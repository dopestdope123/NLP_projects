{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sheet 1","metadata":{"_uuid":"f7936abb-1752-4a94-b822-3fd467713e32","_cell_guid":"989a8189-5df6-43b7-9ff2-2dd47f7d33a6","datalore":{"sheet_delimiter":true},"trusted":true}},{"cell_type":"code","source":"!pip install pyconll","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:45.914131Z","iopub.execute_input":"2022-04-19T16:48:45.914382Z","iopub.status.idle":"2022-04-19T16:48:53.336271Z","shell.execute_reply.started":"2022-04-19T16:48:45.914354Z","shell.execute_reply":"2022-04-19T16:48:53.335414Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyconll in /opt/conda/lib/python3.7/site-packages (3.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertModel, BertConfig,BertTokenizer\nimport nltk\nimport torch\n\nimport numpy as np\nimport pyconll\nimport urllib.request\nimport transformers\nimport torch.nn\nfrom tqdm.notebook import tqdm\nimport torch.optim as optim\nfrom torchtext import data\nimport random\nfrom torchtext.legacy import data\nimport torch.nn as nn","metadata":{"_uuid":"31e8e7f9-07ce-4d20-9955-79b205c1859d","_cell_guid":"42f98df9-535a-4468-a225-3b3af427096a","collapsed":false,"datalore":{"type":"CODE","hide_input_from_viewers":false,"hide_output_from_viewers":false},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-19T16:48:53.340155Z","iopub.execute_input":"2022-04-19T16:48:53.340390Z","iopub.status.idle":"2022-04-19T16:48:53.348184Z","shell.execute_reply.started":"2022-04-19T16:48:53.340363Z","shell.execute_reply":"2022-04-19T16:48:53.347321Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:53.349795Z","iopub.execute_input":"2022-04-19T16:48:53.350061Z","iopub.status.idle":"2022-04-19T16:48:53.357683Z","shell.execute_reply.started":"2022-04-19T16:48:53.350028Z","shell.execute_reply":"2022-04-19T16:48:53.356958Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":"# Load Datset","metadata":{"_uuid":"38b847b6-bfc5-48ca-9f76-a611fa111832","_cell_guid":"5024fbc9-2591-44af-8191-4d1b9852f3f7","datalore":{"type":"MD","hide_input_from_viewers":false,"hide_output_from_viewers":false},"trusted":true}},{"cell_type":"code","source":"def read_conllu(path):\n    data = pyconll.load_from_file(path)\n    tagged_sentences=[]\n    t=0\n    for sentence in data:\n        tagged_sentence=[]\n        for token in sentence:\n            if token.upos and token.form:\n                t+=1\n                tagged_sentence.append((token.form.lower(), token.upos))\n        tagged_sentences.append(tagged_sentence)\n    return tagged_sentences","metadata":{"_uuid":"7ba9910c-602a-409f-b2cb-f8be950d7c4e","_cell_guid":"d4fafbe3-7e34-4da5-9c31-0583d428437e","collapsed":false,"datalore":{"type":"CODE","hide_input_from_viewers":false,"hide_output_from_viewers":false},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-19T16:48:53.360295Z","iopub.execute_input":"2022-04-19T16:48:53.360625Z","iopub.status.idle":"2022-04-19T16:48:53.368174Z","shell.execute_reply.started":"2022-04-19T16:48:53.360590Z","shell.execute_reply":"2022-04-19T16:48:53.367327Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"#train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n#val_sentences = read_conllu(UD_ENGLISH_DEV)\nUD_Breton='../input/posbert/UD_Breton-KEB/UD_Breton-KEB/br_keb-ud-test.conllu'\nUD_Yoruba='../input/posbert/UD_Yoruba-YTB/UD_Yoruba-YTB/yo_ytb-ud-test.conllu'\nUD_Welsh_dev='../input/ud-welshccg/UD_Welsh-CCG/cy_ccg-ud-dev.conllu'\nUD_Welsh_train='../input/ud-welshccg/UD_Welsh-CCG/cy_ccg-ud-train.conllu'\nUD_Welsh_test='../input/ud-welshccg/UD_Welsh-CCG/cy_ccg-ud-test.conllu'\nUD_Wolof_dev='../input/wolof-ud/UD_Wolof-WTB/wo_wtb-ud-dev.conllu'\nUD_Wolof_train='../input/wolof-ud/UD_Wolof-WTB/wo_wtb-ud-train.conllu'\nUD_Wolof_test='../input/wolof-ud/UD_Wolof-WTB/wo_wtb-ud-test.conllu'\nBreton_sent = read_conllu(UD_Breton)\nYoruba_sent = read_conllu(UD_Yoruba)\n#Welsh_dev=read_conllu(UD_Welsh_dev)\n#Welsh_train=read_conllu(UD_Welsh_train)\n#Welsh_test=read_conllu(UD_Welsh_test)\nWolof_dev=read_conllu(UD_Wolof_dev)\nWolof_train=read_conllu(UD_Wolof_train)\nWolof_test=read_conllu(UD_Wolof_test)","metadata":{"_uuid":"c5e54e85-df40-4ddf-9857-8a601bb82904","_cell_guid":"3457e8e4-0025-423a-8566-21139350af95","collapsed":false,"datalore":{"type":"CODE","hide_input_from_viewers":false,"hide_output_from_viewers":false},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-19T16:48:53.369645Z","iopub.execute_input":"2022-04-19T16:48:53.370170Z","iopub.status.idle":"2022-04-19T16:48:54.561750Z","shell.execute_reply.started":"2022-04-19T16:48:53.370134Z","shell.execute_reply":"2022-04-19T16:48:54.560896Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"UD_tags_path ='../input/conllutools/cpos.ud'\nUD_tags_file = open(UD_tags_path, 'r')\nUD_tags = UD_tags_file.readlines()\npostags = []\nfor line in UD_tags[:-1]:\n    if ('#' not in line) and ('' != line):\n        postags.append(line.replace(\"\\n\", \"\"))\n    UD_tags_file.close()\npostags","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.563019Z","iopub.execute_input":"2022-04-19T16:48:54.563326Z","iopub.status.idle":"2022-04-19T16:48:54.576438Z","shell.execute_reply.started":"2022-04-19T16:48:54.563293Z","shell.execute_reply":"2022-04-19T16:48:54.575176Z"},"trusted":true},"execution_count":79,"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"['ADJ',\n 'ADP',\n 'ADV',\n 'AUX',\n 'CCONJ',\n 'DET',\n 'INTJ',\n 'NOUN',\n 'NUM',\n 'PART',\n 'PRON',\n 'PROPN',\n 'PUNCT',\n 'SCONJ',\n 'SYM',\n 'VERB']"},"metadata":{}}]},{"cell_type":"code","source":"\nWolof_sent=Wolof_train+Wolof_test+Wolof_dev","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.577973Z","iopub.execute_input":"2022-04-19T16:48:54.578237Z","iopub.status.idle":"2022-04-19T16:48:54.582386Z","shell.execute_reply.started":"2022-04-19T16:48:54.578201Z","shell.execute_reply":"2022-04-19T16:48:54.581505Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"postags = [\"<pad>\"] + postags\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.583999Z","iopub.execute_input":"2022-04-19T16:48:54.584585Z","iopub.status.idle":"2022-04-19T16:48:54.592433Z","shell.execute_reply.started":"2022-04-19T16:48:54.584547Z","shell.execute_reply":"2022-04-19T16:48:54.591469Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"tag2idx = {tag:idx for idx, tag in enumerate(postags)}\nidx2tag = {idx:tag for idx, tag in enumerate(postags)}","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.593985Z","iopub.execute_input":"2022-04-19T16:48:54.594312Z","iopub.status.idle":"2022-04-19T16:48:54.601946Z","shell.execute_reply.started":"2022-04-19T16:48:54.594272Z","shell.execute_reply":"2022-04-19T16:48:54.601107Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"Wolof_sent=Wolof_sent[0:500]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.605952Z","iopub.execute_input":"2022-04-19T16:48:54.606216Z","iopub.status.idle":"2022-04-19T16:48:54.612332Z","shell.execute_reply.started":"2022-04-19T16:48:54.606186Z","shell.execute_reply":"2022-04-19T16:48:54.611369Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"len(Wolof_sent)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.613920Z","iopub.execute_input":"2022-04-19T16:48:54.614332Z","iopub.status.idle":"2022-04-19T16:48:54.624451Z","shell.execute_reply.started":"2022-04-19T16:48:54.614294Z","shell.execute_reply":"2022-04-19T16:48:54.623706Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"500"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nbr_train_data, br_test_data = train_test_split(Breton_sent, test_size=.25)\n#wa_train_data, wa_test_data=train_test_split(Welsh_sent, test_size=.3)\nwo_train_data, wo_test_data=train_test_split(Wolof_sent, test_size=.25)\nyo_train_data, yo_test_data=train_test_split(Yoruba_sent, test_size=.25)\nlen(wo_train_data), len(wo_test_data)","metadata":{"_uuid":"61d3837f-723e-439f-bf33-438fef613112","_cell_guid":"9a5290a6-f1af-4741-9e16-468b785da476","collapsed":false,"datalore":{"type":"CODE","hide_input_from_viewers":false,"hide_output_from_viewers":false},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-19T16:48:54.626971Z","iopub.execute_input":"2022-04-19T16:48:54.627217Z","iopub.status.idle":"2022-04-19T16:48:54.641125Z","shell.execute_reply.started":"2022-04-19T16:48:54.627193Z","shell.execute_reply":"2022-04-19T16:48:54.640261Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"(375, 125)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"yo_train_data, yo_test_data = train_test_split(Yoruba_sent, test_size=.3)\nlen(yo_train_data), len(yo_test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.642307Z","iopub.execute_input":"2022-04-19T16:48:54.643252Z","iopub.status.idle":"2022-04-19T16:48:54.655235Z","shell.execute_reply.started":"2022-04-19T16:48:54.643213Z","shell.execute_reply":"2022-04-19T16:48:54.654365Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"(222, 96)"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:54.657649Z","iopub.execute_input":"2022-04-19T16:48:54.657940Z","iopub.status.idle":"2022-04-19T16:48:54.664444Z","shell.execute_reply.started":"2022-04-19T16:48:54.657903Z","shell.execute_reply":"2022-04-19T16:48:54.663632Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)","metadata":{"_uuid":"3012b29d-7d97-4314-8a2d-c0b426a55daf","_cell_guid":"7b7cfcfd-4341-4d07-b919-d02e49056018","collapsed":false,"datalore":{"type":"CODE","hide_input_from_viewers":false,"hide_output_from_viewers":false},"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-04-19T16:48:54.665853Z","iopub.execute_input":"2022-04-19T16:48:54.666116Z","iopub.status.idle":"2022-04-19T16:48:57.273659Z","shell.execute_reply.started":"2022-04-19T16:48:54.666082Z","shell.execute_reply":"2022-04-19T16:48:57.272826Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"class PosDataset(data.Dataset):\n    def __init__(self, tagged_sents):\n        sents, tags_li = [], [] # list of lists\n        for sent in tagged_sents:\n            words = [word_pos[0] for word_pos in sent]\n            tags = [word_pos[1] for word_pos in sent]\n            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n        self.sents, self.tags_li = sents, tags_li\n\n    def __len__(self):\n        return len(self.sents)\n\n    def __getitem__(self, idx):\n        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n\n       \n        x, y = [], [] # list of ids\n        is_heads = [] # list. 1: the token is the first piece of a word\n        for w, t in zip(words, tags):\n            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n            xx = tokenizer.convert_tokens_to_ids(tokens)\n\n            is_head = [1] + [0]*(len(tokens) - 1)\n\n            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n            yy = [tag2idx[each] for each in t]  # (T,)\n\n            x.extend(xx)\n            is_heads.extend(is_head)\n            y.extend(yy)\n\n        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n\n        # seqlen\n        seqlen = len(y)\n\n        # to string\n        words = \" \".join(words)\n        tags = \" \".join(tags)\n        return words, x, is_heads, tags, y, seqlen\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:57.274906Z","iopub.execute_input":"2022-04-19T16:48:57.276921Z","iopub.status.idle":"2022-04-19T16:48:57.288733Z","shell.execute_reply.started":"2022-04-19T16:48:57.276878Z","shell.execute_reply":"2022-04-19T16:48:57.287454Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(batch):\n    '''Pads to the longest sample'''\n    f = lambda x: [sample[x] for sample in batch]\n    words = f(0)\n    is_heads = f(2)\n    tags = f(3)\n    seqlens = f(-1)\n    maxlen = np.array(seqlens).max()\n\n    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n    x = f(1, maxlen)\n    y = f(-2, maxlen)\n\n\n    f = torch.LongTensor\n\n    return words, f(x), is_heads, tags, f(y), seqlens","metadata":{"execution":{"iopub.status.busy":"2022-04-19T16:48:57.289988Z","iopub.execute_input":"2022-04-19T16:48:57.290800Z","iopub.status.idle":"2022-04-19T16:48:57.303042Z","shell.execute_reply.started":"2022-04-19T16:48:57.290756Z","shell.execute_reply":"2022-04-19T16:48:57.302398Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self, modeltype='bert-base-multilingual-cased',vocab_size=None):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(modeltype,return_dict=False)\n        self.fc = nn.Linear(768, vocab_size)\n        self.device = device\n\n    def forward(self, x, y):\n        '''\n        x: (N, T). int64\n        y: (N, T). int64\n        '''\n        x = x.to(device)\n        y = y.to(device)\n        \n        if self.training:\n            self.bert.train()\n            encoded_layers, _ = self.bert(x)\n            enc = encoded_layers[-1]\n        else:\n            self.bert.eval()\n            with torch.no_grad():\n                encoded_layers, _ = self.bert(x)\n                enc = encoded_layers[-1]\n        \n        logits = self.fc(enc)\n        y_hat = logits.argmax(-1)\n        return logits, y, y_hat","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:07:54.101993Z","iopub.execute_input":"2022-04-19T17:07:54.102305Z","iopub.status.idle":"2022-04-19T17:07:54.117975Z","shell.execute_reply.started":"2022-04-19T17:07:54.102266Z","shell.execute_reply":"2022-04-19T17:07:54.117211Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator):\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n    loss_list={}\n    for i, batch in enumerate(iterator):\n        words, x, is_heads, tags, y, seqlens = batch\n        _y = y # for monitoring\n        optimizer.zero_grad()\n        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n        logits = logits.view((-1), logits.shape[-1]) # (N*T, VOCAB)\n        y = y.view(-1)  # (N*T,)\n       \n       # print(torch.Size(logits))\n       # print(torch.Size(y))\n        loss = criterion(logits, y)\n        loss.backward()\n\n        optimizer.step()\n        \n        if i%10==0: # monitoring\n            print(\"step: {}, loss: {}\".format(i, loss.item()))\n            loss_list[i]=loss.item()\n    return loss_list ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:02:08.553292Z","iopub.execute_input":"2022-04-19T17:02:08.553982Z","iopub.status.idle":"2022-04-19T17:02:08.561767Z","shell.execute_reply.started":"2022-04-19T17:02:08.553945Z","shell.execute_reply":"2022-04-19T17:02:08.561055Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"def eval(model, iterator):\n    model.eval()\n\n    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n    with torch.no_grad():\n        for i, batch in enumerate(iterator):\n            words, x, is_heads, tags, y, seqlens = batch\n\n            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n            \n            Words.extend(words)\n            Is_heads.extend(is_heads)\n            Tags.extend(tags)\n            Y.extend(y.numpy().tolist())\n            Y_hat.append(y_hat.cpu().numpy().tolist())\n    ## gets results and save\n    with open(\"result\", 'w') as fout:\n        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n           # y_hat=y_hat[1:-1]\n            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n            preds = [idx2tag[hat] for hat in y_hat]\n            #assert len(preds)==len(words.split())==len(tags.split())\n            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n                fout.write(\"{} {} {}\\n\".format(w, t, p))\n            fout.write(\"\\n\")\n            \n    ## calc metric\n    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n\n    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n    print(acc)\n    return acc ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:02:32.555761Z","iopub.execute_input":"2022-04-19T17:02:32.556023Z","iopub.status.idle":"2022-04-19T17:02:32.567496Z","shell.execute_reply.started":"2022-04-19T17:02:32.555995Z","shell.execute_reply":"2022-04-19T17:02:32.566805Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel_wo = Net(vocab_size=len(tag2idx))\nmodel_wo.to(device)\nmodel_wo = nn.DataParallel(model_wo)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:37:30.668782Z","iopub.execute_input":"2022-04-19T17:37:30.669037Z","iopub.status.idle":"2022-04-19T17:37:33.692729Z","shell.execute_reply.started":"2022-04-19T17:37:30.669010Z","shell.execute_reply":"2022-04-19T17:37:33.691949Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils import data\nwo_train_dataset = PosDataset(wo_train_data)\nwo_eval_dataset = PosDataset(wo_test_data)\nyo_train_dataset = PosDataset(yo_train_data)\nyo_eval_dataset = PosDataset(yo_test_data)\n\n\n\nyo_train_iter = data.DataLoader(dataset=yo_train_dataset,\n                             batch_size=1,\n                             shuffle=True,\n                             num_workers=1,\n                             collate_fn=pad)\nyo_test_iter = data.DataLoader(dataset=yo_eval_dataset,\n                             batch_size=1,\n                             shuffle=False,\n                             num_workers=1,\n                             collate_fn=pad)\nwo_train_iter = data.DataLoader(dataset=wo_train_dataset,\n                             batch_size=1,\n                             shuffle=True,\n                             num_workers=1,\n                             collate_fn=pad)\nwo_test_iter = data.DataLoader(dataset=wo_eval_dataset,\n                             batch_size=1,\n                             shuffle=False,\n                             num_workers=1,\n                             collate_fn=pad)\n\n\n#optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:37:35.552295Z","iopub.execute_input":"2022-04-19T17:37:35.552967Z","iopub.status.idle":"2022-04-19T17:37:35.568201Z","shell.execute_reply.started":"2022-04-19T17:37:35.552823Z","shell.execute_reply":"2022-04-19T17:37:35.567352Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Epoch=10\nwo_acc=[]\nwo_train=[]\ni=0\nwhile i<10: \n    wo_train.append(train(model_wo, wo_train_iter))\n    wo_acc.append(eval(model_wo, wo_test_iter))\n    i+=1\nprint(wo_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:37:37.321170Z","iopub.execute_input":"2022-04-19T17:37:37.321595Z","iopub.status.idle":"2022-04-19T17:41:14.254203Z","shell.execute_reply.started":"2022-04-19T17:37:37.321558Z","shell.execute_reply":"2022-04-19T17:41:14.253345Z"},"trusted":true},"execution_count":175,"outputs":[{"name":"stdout","text":"step: 0, loss: 2.888134717941284\nstep: 10, loss: 1.6356077194213867\nstep: 20, loss: 1.0385125875473022\nstep: 30, loss: 0.9337869882583618\nstep: 40, loss: 0.5508790016174316\nstep: 50, loss: 0.7170218825340271\nstep: 60, loss: 0.8036861419677734\nstep: 70, loss: 0.7716289162635803\nstep: 80, loss: 0.5254815816879272\nstep: 90, loss: 0.7356905341148376\nstep: 100, loss: 0.7536472678184509\nstep: 110, loss: 0.5410031080245972\nstep: 120, loss: 0.3987736701965332\nstep: 130, loss: 0.36924830079078674\nstep: 140, loss: 0.29116639494895935\nstep: 150, loss: 0.533483624458313\nstep: 160, loss: 0.7549551129341125\nstep: 170, loss: 0.7114834785461426\nstep: 180, loss: 0.4441230297088623\nstep: 190, loss: 0.85006183385849\nstep: 200, loss: 0.5255175828933716\nstep: 210, loss: 0.6010774374008179\nstep: 220, loss: 0.33465614914894104\nstep: 230, loss: 0.2987624704837799\nstep: 240, loss: 0.7021782398223877\nstep: 250, loss: 0.2909950911998749\nstep: 260, loss: 0.2948327362537384\nstep: 270, loss: 0.40566813945770264\nstep: 280, loss: 0.2739885151386261\nstep: 290, loss: 0.0807894691824913\nstep: 300, loss: 0.2092217057943344\nstep: 310, loss: 0.5492917895317078\nstep: 320, loss: 0.14021077752113342\nstep: 330, loss: 0.20763827860355377\nstep: 340, loss: 0.38043880462646484\nstep: 350, loss: 0.367337167263031\nstep: 360, loss: 0.27082768082618713\nstep: 370, loss: 0.6456416249275208\n0.7801784875528417\nstep: 0, loss: 0.41254258155822754\nstep: 10, loss: 0.15864087641239166\nstep: 20, loss: 0.325490802526474\nstep: 30, loss: 0.3957895040512085\nstep: 40, loss: 0.17882347106933594\nstep: 50, loss: 0.5012397766113281\nstep: 60, loss: 0.526500403881073\nstep: 70, loss: 0.5396782755851746\nstep: 80, loss: 0.42810001969337463\nstep: 90, loss: 0.5570428371429443\nstep: 100, loss: 0.27907902002334595\nstep: 110, loss: 0.46329861879348755\nstep: 120, loss: 0.6085507273674011\nstep: 130, loss: 0.0839102491736412\nstep: 140, loss: 0.2621765732765198\nstep: 150, loss: 0.029251856729388237\nstep: 160, loss: 0.3841005563735962\nstep: 170, loss: 0.33429741859436035\nstep: 180, loss: 0.27710261940956116\nstep: 190, loss: 0.38039082288742065\nstep: 200, loss: 0.36629652976989746\nstep: 210, loss: 0.6946713924407959\nstep: 220, loss: 0.06545711308717728\nstep: 230, loss: 0.21505305171012878\nstep: 240, loss: 0.10332518070936203\nstep: 250, loss: 0.25131669640541077\nstep: 260, loss: 0.3061816990375519\nstep: 270, loss: 0.31978434324264526\nstep: 280, loss: 0.6816893219947815\nstep: 290, loss: 0.09187479317188263\nstep: 300, loss: 0.21031425893306732\nstep: 310, loss: 0.18969756364822388\nstep: 320, loss: 0.37684738636016846\nstep: 330, loss: 0.11161542683839798\nstep: 340, loss: 0.41981834173202515\nstep: 350, loss: 0.25661829113960266\nstep: 360, loss: 0.04593697562813759\nstep: 370, loss: 0.18725021183490753\n0.8304368248003757\nstep: 0, loss: 0.09460145980119705\nstep: 10, loss: 0.09375046193599701\nstep: 20, loss: 0.11198485642671585\nstep: 30, loss: 0.14817476272583008\nstep: 40, loss: 0.14285963773727417\nstep: 50, loss: 0.18911932408809662\nstep: 60, loss: 0.12407772243022919\nstep: 70, loss: 0.043073851615190506\nstep: 80, loss: 0.28642240166664124\nstep: 90, loss: 0.06720461696386337\nstep: 100, loss: 0.313504695892334\nstep: 110, loss: 0.24899670481681824\nstep: 120, loss: 0.050689052790403366\nstep: 130, loss: 0.06157397851347923\nstep: 140, loss: 0.4245433509349823\nstep: 150, loss: 0.2884478271007538\nstep: 160, loss: 0.2811897397041321\nstep: 170, loss: 0.15267901122570038\nstep: 180, loss: 0.12367825210094452\nstep: 190, loss: 0.2424941062927246\nstep: 200, loss: 0.08134869486093521\nstep: 210, loss: 0.32557663321495056\nstep: 220, loss: 0.3019826114177704\nstep: 230, loss: 0.292866587638855\nstep: 240, loss: 0.25712329149246216\nstep: 250, loss: 0.5726699233055115\nstep: 260, loss: 0.18893864750862122\nstep: 270, loss: 0.17415502667427063\nstep: 280, loss: 0.6662222146987915\nstep: 290, loss: 0.05931062996387482\nstep: 300, loss: 0.26343995332717896\nstep: 310, loss: 0.6322254538536072\nstep: 320, loss: 0.167559415102005\nstep: 330, loss: 0.10367990285158157\nstep: 340, loss: 0.35343876481056213\nstep: 350, loss: 0.3905445337295532\nstep: 360, loss: 0.27499234676361084\nstep: 370, loss: 0.17384392023086548\n0.8337247534053547\nstep: 0, loss: 0.035148315131664276\nstep: 10, loss: 0.22532406449317932\nstep: 20, loss: 0.12298566848039627\nstep: 30, loss: 0.05373772978782654\nstep: 40, loss: 0.018308598548173904\nstep: 50, loss: 0.1894685924053192\nstep: 60, loss: 0.1287219524383545\nstep: 70, loss: 0.02258177474141121\nstep: 80, loss: 0.1760118007659912\nstep: 90, loss: 0.9267315864562988\nstep: 100, loss: 0.2216448336839676\nstep: 110, loss: 0.2806888520717621\nstep: 120, loss: 0.4801079034805298\nstep: 130, loss: 0.27438387274742126\nstep: 140, loss: 0.22617107629776\nstep: 150, loss: 0.006375573109835386\nstep: 160, loss: 0.02240890823304653\nstep: 170, loss: 0.4715788960456848\nstep: 180, loss: 0.24560798704624176\nstep: 190, loss: 0.4004358947277069\nstep: 200, loss: 0.2065865695476532\nstep: 210, loss: 0.29251936078071594\nstep: 220, loss: 0.03687913715839386\nstep: 230, loss: 0.10772524774074554\nstep: 240, loss: 0.11948435753583908\nstep: 250, loss: 0.07907731086015701\nstep: 260, loss: 0.0801587924361229\nstep: 270, loss: 0.10869164764881134\nstep: 280, loss: 0.031488947570323944\nstep: 290, loss: 0.43647322058677673\nstep: 300, loss: 0.3440166413784027\nstep: 310, loss: 0.15880848467350006\nstep: 320, loss: 0.21970826387405396\nstep: 330, loss: 0.2795480787754059\nstep: 340, loss: 0.14216424524784088\nstep: 350, loss: 0.044684190303087234\nstep: 360, loss: 0.028840694576501846\nstep: 370, loss: 0.024369798600673676\n0.8496946923438234\nstep: 0, loss: 0.18580247461795807\nstep: 10, loss: 0.16806258261203766\nstep: 20, loss: 0.3925178647041321\nstep: 30, loss: 0.01999146305024624\nstep: 40, loss: 0.12795715034008026\nstep: 50, loss: 0.3689028024673462\nstep: 60, loss: 0.14865261316299438\nstep: 70, loss: 0.10605239868164062\nstep: 80, loss: 0.12140219658613205\nstep: 90, loss: 0.004209943115711212\nstep: 100, loss: 0.3217039704322815\nstep: 110, loss: 0.007578480057418346\nstep: 120, loss: 0.004707681946456432\nstep: 130, loss: 0.4666081964969635\nstep: 140, loss: 0.14563797414302826\nstep: 150, loss: 0.18330000340938568\nstep: 160, loss: 0.16296476125717163\nstep: 170, loss: 0.005451511591672897\nstep: 180, loss: 0.3028508126735687\nstep: 190, loss: 0.027470501139760017\nstep: 200, loss: 0.025071481242775917\nstep: 210, loss: 0.16746550798416138\nstep: 220, loss: 0.1511048972606659\nstep: 230, loss: 0.03752215579152107\nstep: 240, loss: 0.22974126040935516\nstep: 250, loss: 0.062433283776044846\nstep: 260, loss: 0.3297782242298126\nstep: 270, loss: 0.1283036172389984\nstep: 280, loss: 0.01863192766904831\nstep: 290, loss: 0.20290520787239075\nstep: 300, loss: 0.10814837366342545\nstep: 310, loss: 0.3512554466724396\nstep: 320, loss: 0.41634970903396606\nstep: 330, loss: 0.16513390839099884\nstep: 340, loss: 0.046634674072265625\nstep: 350, loss: 0.2837412655353546\nstep: 360, loss: 0.17807909846305847\nstep: 370, loss: 0.5594281554222107\n0.8581493658994833\nstep: 0, loss: 0.10336708277463913\nstep: 10, loss: 0.2669711709022522\nstep: 20, loss: 0.026168830692768097\nstep: 30, loss: 0.3876245617866516\nstep: 40, loss: 0.0347006730735302\nstep: 50, loss: 0.1811663955450058\nstep: 60, loss: 0.019012508913874626\nstep: 70, loss: 0.36733925342559814\nstep: 80, loss: 0.1774439811706543\nstep: 90, loss: 0.10543330013751984\nstep: 100, loss: 0.23040249943733215\nstep: 110, loss: 0.01996484212577343\nstep: 120, loss: 0.12874417006969452\nstep: 130, loss: 0.40856435894966125\nstep: 140, loss: 0.010544003918766975\nstep: 150, loss: 0.05832540988922119\nstep: 160, loss: 0.30270087718963623\nstep: 170, loss: 0.2683263421058655\nstep: 180, loss: 0.2477518767118454\nstep: 190, loss: 0.06497698277235031\nstep: 200, loss: 0.16014575958251953\nstep: 210, loss: 0.195707768201828\nstep: 220, loss: 0.01831522211432457\nstep: 230, loss: 0.07667860388755798\nstep: 240, loss: 0.29015055298805237\nstep: 250, loss: 0.08138982206583023\nstep: 260, loss: 0.025551091879606247\nstep: 270, loss: 0.12863686680793762\nstep: 280, loss: 0.21730639040470123\nstep: 290, loss: 0.07292226701974869\nstep: 300, loss: 0.3017060160636902\nstep: 310, loss: 0.1890035718679428\nstep: 320, loss: 0.32805323600769043\nstep: 330, loss: 0.056373171508312225\nstep: 340, loss: 0.02215486951172352\nstep: 350, loss: 0.28501519560813904\nstep: 360, loss: 0.046123966574668884\nstep: 370, loss: 0.1409628540277481\n0.8492249882573979\nstep: 0, loss: 0.00719436863437295\nstep: 10, loss: 0.23980002105236053\nstep: 20, loss: 0.05767308920621872\nstep: 30, loss: 0.11417461186647415\nstep: 40, loss: 0.02304166741669178\nstep: 50, loss: 0.0048482599668204784\nstep: 60, loss: 0.09279176592826843\nstep: 70, loss: 0.005494512151926756\nstep: 80, loss: 0.19862882792949677\nstep: 90, loss: 0.11015013605356216\nstep: 100, loss: 0.007367666810750961\nstep: 110, loss: 0.16485591232776642\nstep: 120, loss: 0.04419397562742233\nstep: 130, loss: 0.015919756144285202\nstep: 140, loss: 0.0601094551384449\nstep: 150, loss: 0.11735851317644119\nstep: 160, loss: 0.004558650776743889\nstep: 170, loss: 0.003480921033769846\nstep: 180, loss: 0.19707627594470978\nstep: 190, loss: 0.07900640368461609\nstep: 200, loss: 0.051049165427684784\nstep: 210, loss: 0.016239717602729797\nstep: 220, loss: 0.0225067101418972\nstep: 230, loss: 0.09906591475009918\nstep: 240, loss: 0.009516441263258457\nstep: 250, loss: 0.004918402526527643\nstep: 260, loss: 0.03642560541629791\nstep: 270, loss: 0.027712522074580193\nstep: 280, loss: 0.17049960792064667\nstep: 290, loss: 0.2084895521402359\nstep: 300, loss: 0.016775324940681458\nstep: 310, loss: 0.12499027699232101\nstep: 320, loss: 1.0472190380096436\nstep: 330, loss: 0.6717585325241089\nstep: 340, loss: 0.11147152632474899\nstep: 350, loss: 0.15118254721164703\nstep: 360, loss: 0.307923823595047\nstep: 370, loss: 0.33170315623283386\n0.7674964772193518\nstep: 0, loss: 0.47051626443862915\nstep: 10, loss: 0.19372840225696564\nstep: 20, loss: 0.3718441426753998\nstep: 30, loss: 0.24554036557674408\nstep: 40, loss: 0.241196408867836\nstep: 50, loss: 0.2860710322856903\nstep: 60, loss: 0.25834810733795166\nstep: 70, loss: 0.5679279565811157\nstep: 80, loss: 0.16981524229049683\nstep: 90, loss: 0.34720945358276367\nstep: 100, loss: 0.3228173553943634\nstep: 110, loss: 0.6512008905410767\nstep: 120, loss: 0.06719125807285309\nstep: 130, loss: 0.011090612970292568\nstep: 140, loss: 0.22577853500843048\nstep: 150, loss: 0.22188642621040344\nstep: 160, loss: 0.07925235480070114\nstep: 170, loss: 0.09835388511419296\nstep: 180, loss: 0.026054108515381813\nstep: 190, loss: 0.1844756007194519\nstep: 200, loss: 0.03861401602625847\nstep: 210, loss: 0.016530467197299004\nstep: 220, loss: 0.28900840878486633\nstep: 230, loss: 0.0796278640627861\nstep: 240, loss: 0.26393839716911316\nstep: 250, loss: 0.18248441815376282\nstep: 260, loss: 0.15618176758289337\nstep: 270, loss: 0.09604556113481522\nstep: 280, loss: 0.10017062723636627\nstep: 290, loss: 0.11714445799589157\nstep: 300, loss: 0.049715351313352585\nstep: 310, loss: 0.12609705328941345\nstep: 320, loss: 0.08409364521503448\nstep: 330, loss: 0.058592360466718674\nstep: 340, loss: 0.017604481428861618\nstep: 350, loss: 0.43427014350891113\nstep: 360, loss: 0.0466105118393898\nstep: 370, loss: 0.10032916069030762\n0.8567402536402067\nstep: 0, loss: 0.02847863920032978\nstep: 10, loss: 0.004576089791953564\nstep: 20, loss: 0.19345031678676605\nstep: 30, loss: 0.15666866302490234\nstep: 40, loss: 0.31042104959487915\nstep: 50, loss: 0.12030558288097382\nstep: 60, loss: 0.09154269844293594\nstep: 70, loss: 0.21778735518455505\nstep: 80, loss: 0.007571839261800051\nstep: 90, loss: 0.003824560670182109\nstep: 100, loss: 0.14922037720680237\nstep: 110, loss: 0.05691496655344963\nstep: 120, loss: 0.018941299989819527\nstep: 130, loss: 0.0824771374464035\nstep: 140, loss: 0.012380958534777164\nstep: 150, loss: 0.01268965844064951\nstep: 160, loss: 0.07940926402807236\nstep: 170, loss: 0.013193346560001373\nstep: 180, loss: 0.35275694727897644\nstep: 190, loss: 0.2217976301908493\nstep: 200, loss: 0.07299890369176865\nstep: 210, loss: 0.2911509871482849\nstep: 220, loss: 0.3912443518638611\nstep: 230, loss: 0.38594749569892883\nstep: 240, loss: 0.24325606226921082\nstep: 250, loss: 0.5999535918235779\nstep: 260, loss: 0.33967626094818115\nstep: 270, loss: 0.13948580622673035\nstep: 280, loss: 0.2637943625450134\nstep: 290, loss: 0.42445677518844604\nstep: 300, loss: 0.0154866399243474\nstep: 310, loss: 0.09668407589197159\nstep: 320, loss: 0.33439958095550537\nstep: 330, loss: 0.036113426089286804\nstep: 340, loss: 0.014548251405358315\nstep: 350, loss: 0.25282156467437744\nstep: 360, loss: 0.12842395901679993\nstep: 370, loss: 0.051722098141908646\n0.8539220291216534\nstep: 0, loss: 0.2165626585483551\nstep: 10, loss: 0.06051550433039665\nstep: 20, loss: 0.16075164079666138\nstep: 30, loss: 0.1120043396949768\nstep: 40, loss: 0.035931747406721115\nstep: 50, loss: 0.03682081401348114\nstep: 60, loss: 0.0788891464471817\nstep: 70, loss: 0.006484764628112316\nstep: 80, loss: 0.11436891555786133\nstep: 90, loss: 0.14631617069244385\nstep: 100, loss: 0.3183581531047821\nstep: 110, loss: 0.17245711386203766\nstep: 120, loss: 0.02728031389415264\nstep: 130, loss: 0.7102425694465637\nstep: 140, loss: 0.19428405165672302\nstep: 150, loss: 0.2947438359260559\nstep: 160, loss: 0.3678797781467438\nstep: 170, loss: 0.14469340443611145\nstep: 180, loss: 0.06843788921833038\nstep: 190, loss: 0.1694156676530838\nstep: 200, loss: 0.01302268635481596\nstep: 210, loss: 0.22755159437656403\nstep: 220, loss: 0.24498529732227325\nstep: 230, loss: 0.1893741339445114\nstep: 240, loss: 0.14569267630577087\nstep: 250, loss: 0.12523667514324188\nstep: 260, loss: 0.048267778009176254\nstep: 270, loss: 0.017401916906237602\nstep: 280, loss: 0.07095449417829514\nstep: 290, loss: 0.18437033891677856\nstep: 300, loss: 0.4046191871166229\nstep: 310, loss: 0.03274858370423317\nstep: 320, loss: 0.045760802924633026\nstep: 330, loss: 0.005456629674881697\nstep: 340, loss: 0.17396271228790283\nstep: 350, loss: 0.023876873776316643\nstep: 360, loss: 0.18284232914447784\nstep: 370, loss: 0.009852676652371883\n0.8576796618130578\n[0.7801784875528417, 0.8304368248003757, 0.8337247534053547, 0.8496946923438234, 0.8581493658994833, 0.8492249882573979, 0.7674964772193518, 0.8567402536402067, 0.8539220291216534, 0.8576796618130578]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model_yowo = Net(modeltype='Davlan/bert-base-multilingual-cased-finetuned-yoruba',vocab_size=len(tag2idx))\nmodel_yowo.to(device)\nmodel_yowo = nn.DataParallel(model_yowo)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:19.513214Z","iopub.execute_input":"2022-04-19T17:41:19.513500Z","iopub.status.idle":"2022-04-19T17:41:22.403619Z","shell.execute_reply.started":"2022-04-19T17:41:19.513469Z","shell.execute_reply":"2022-04-19T17:41:22.402823Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-yoruba were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-yoruba and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"Epoch=10\nyowo_acc=[]\nyowo_train=[]\ni=0\nwhile i<10: \n    yowo_train.append(train(model_yowo, wo_train_iter))\n    yowo_acc.append(eval(model_yowo, wo_test_iter))\n    i+=1\n\nprint(yowo_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:41:25.933479Z","iopub.execute_input":"2022-04-19T17:41:25.934339Z","iopub.status.idle":"2022-04-19T17:45:04.414236Z","shell.execute_reply.started":"2022-04-19T17:41:25.934292Z","shell.execute_reply":"2022-04-19T17:45:04.413454Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"step: 0, loss: 2.9741899967193604\nstep: 10, loss: 1.3345009088516235\nstep: 20, loss: 1.1162786483764648\nstep: 30, loss: 0.5463511943817139\nstep: 40, loss: 0.9640694260597229\nstep: 50, loss: 0.4672936201095581\nstep: 60, loss: 0.8332943916320801\nstep: 70, loss: 0.4982053339481354\nstep: 80, loss: 0.9978530406951904\nstep: 90, loss: 0.6925190091133118\nstep: 100, loss: 0.7495042681694031\nstep: 110, loss: 0.7440182566642761\nstep: 120, loss: 0.7413892149925232\nstep: 130, loss: 0.4013998210430145\nstep: 140, loss: 0.6531165242195129\nstep: 150, loss: 0.5924021601676941\nstep: 160, loss: 0.6391058564186096\nstep: 170, loss: 0.3855753242969513\nstep: 180, loss: 0.6748175024986267\nstep: 190, loss: 0.480172336101532\nstep: 200, loss: 0.05215876176953316\nstep: 210, loss: 0.48349303007125854\nstep: 220, loss: 0.4072108864784241\nstep: 230, loss: 0.3941448926925659\nstep: 240, loss: 0.3866940140724182\nstep: 250, loss: 0.7865357398986816\nstep: 260, loss: 0.4568943381309509\nstep: 270, loss: 0.31942814588546753\nstep: 280, loss: 0.5100241303443909\nstep: 290, loss: 0.500853955745697\nstep: 300, loss: 0.3202568292617798\nstep: 310, loss: 0.06827786564826965\nstep: 320, loss: 0.36096230149269104\nstep: 330, loss: 0.7812836170196533\nstep: 340, loss: 0.2741739749908447\nstep: 350, loss: 0.6135515570640564\nstep: 360, loss: 0.4743313789367676\nstep: 370, loss: 0.09204355627298355\n0.769375293565054\nstep: 0, loss: 0.4697834253311157\nstep: 10, loss: 0.34179016947746277\nstep: 20, loss: 0.09530659765005112\nstep: 30, loss: 0.6577609181404114\nstep: 40, loss: 0.26084277033805847\nstep: 50, loss: 0.12091788649559021\nstep: 60, loss: 0.16539451479911804\nstep: 70, loss: 0.22930027544498444\nstep: 80, loss: 0.1138353943824768\nstep: 90, loss: 0.13347741961479187\nstep: 100, loss: 0.16660550236701965\nstep: 110, loss: 0.10840073972940445\nstep: 120, loss: 0.16058292984962463\nstep: 130, loss: 0.305085688829422\nstep: 140, loss: 0.39381253719329834\nstep: 150, loss: 0.3111168146133423\nstep: 160, loss: 0.6207773089408875\nstep: 170, loss: 0.1028876081109047\nstep: 180, loss: 0.14521652460098267\nstep: 190, loss: 0.5554053783416748\nstep: 200, loss: 0.23765844106674194\nstep: 210, loss: 0.18713177740573883\nstep: 220, loss: 0.27233266830444336\nstep: 230, loss: 0.12521518766880035\nstep: 240, loss: 0.02189152128994465\nstep: 250, loss: 0.057696543633937836\nstep: 260, loss: 0.4056946337223053\nstep: 270, loss: 0.03983721882104874\nstep: 280, loss: 0.24139782786369324\nstep: 290, loss: 0.22956927120685577\nstep: 300, loss: 0.0239325612783432\nstep: 310, loss: 0.19719716906547546\nstep: 320, loss: 0.37474173307418823\nstep: 330, loss: 0.16982971131801605\nstep: 340, loss: 0.49115943908691406\nstep: 350, loss: 0.18309995532035828\nstep: 360, loss: 0.2865782678127289\nstep: 370, loss: 0.1808651238679886\n0.8191639267261626\nstep: 0, loss: 0.3069532513618469\nstep: 10, loss: 0.15384306013584137\nstep: 20, loss: 0.17084486782550812\nstep: 30, loss: 0.1558997929096222\nstep: 40, loss: 0.16848625242710114\nstep: 50, loss: 0.4845415949821472\nstep: 60, loss: 0.3957422971725464\nstep: 70, loss: 0.06524151563644409\nstep: 80, loss: 0.13703128695487976\nstep: 90, loss: 0.1804843246936798\nstep: 100, loss: 0.5074853301048279\nstep: 110, loss: 0.02628452517092228\nstep: 120, loss: 0.08749893307685852\nstep: 130, loss: 0.12832742929458618\nstep: 140, loss: 0.16310346126556396\nstep: 150, loss: 0.3002516031265259\nstep: 160, loss: 0.013198972679674625\nstep: 170, loss: 0.44479992985725403\nstep: 180, loss: 0.2450224757194519\nstep: 190, loss: 0.28347116708755493\nstep: 200, loss: 0.1955830603837967\nstep: 210, loss: 0.2106303572654724\nstep: 220, loss: 0.10388056188821793\nstep: 230, loss: 0.06526686251163483\nstep: 240, loss: 0.17196951806545258\nstep: 250, loss: 0.4169856607913971\nstep: 260, loss: 0.15465350449085236\nstep: 270, loss: 0.025067271664738655\nstep: 280, loss: 0.2281494140625\nstep: 290, loss: 0.4465792775154114\nstep: 300, loss: 0.013131167739629745\nstep: 310, loss: 0.20273815095424652\nstep: 320, loss: 0.05638895183801651\nstep: 330, loss: 0.7437770962715149\nstep: 340, loss: 0.09531725198030472\nstep: 350, loss: 0.1989898979663849\nstep: 360, loss: 0.3229735791683197\nstep: 370, loss: 0.166910782456398\n0.8642555190230155\nstep: 0, loss: 0.15925584733486176\nstep: 10, loss: 0.036325886845588684\nstep: 20, loss: 0.3979739844799042\nstep: 30, loss: 0.18374067544937134\nstep: 40, loss: 0.04277867451310158\nstep: 50, loss: 0.032888296991586685\nstep: 60, loss: 0.0247354656457901\nstep: 70, loss: 0.10226225852966309\nstep: 80, loss: 0.1722833216190338\nstep: 90, loss: 0.15533876419067383\nstep: 100, loss: 0.12310268729925156\nstep: 110, loss: 0.2103394716978073\nstep: 120, loss: 0.006868996191769838\nstep: 130, loss: 0.3400220572948456\nstep: 140, loss: 0.19171686470508575\nstep: 150, loss: 0.280437707901001\nstep: 160, loss: 0.2886051535606384\nstep: 170, loss: 0.0348198376595974\nstep: 180, loss: 0.05320050194859505\nstep: 190, loss: 0.31881433725357056\nstep: 200, loss: 0.13955026865005493\nstep: 210, loss: 0.1587652862071991\nstep: 220, loss: 0.2641270160675049\nstep: 230, loss: 0.32696807384490967\nstep: 240, loss: 0.32338860630989075\nstep: 250, loss: 0.20231449604034424\nstep: 260, loss: 0.1434384137392044\nstep: 270, loss: 0.1084890142083168\nstep: 280, loss: 0.4344390034675598\nstep: 290, loss: 0.3162350654602051\nstep: 300, loss: 0.03147651255130768\nstep: 310, loss: 0.050889965146780014\nstep: 320, loss: 0.0866527110338211\nstep: 330, loss: 0.20132578909397125\nstep: 340, loss: 0.10778961330652237\nstep: 350, loss: 0.2120673656463623\nstep: 360, loss: 0.21824374794960022\nstep: 370, loss: 0.09790556132793427\n0.8656646312822922\nstep: 0, loss: 0.3421287536621094\nstep: 10, loss: 0.03344917669892311\nstep: 20, loss: 0.007953990250825882\nstep: 30, loss: 0.2978968918323517\nstep: 40, loss: 0.09794701635837555\nstep: 50, loss: 0.038361500948667526\nstep: 60, loss: 0.463623046875\nstep: 70, loss: 0.006115376949310303\nstep: 80, loss: 0.23858924210071564\nstep: 90, loss: 0.6304240822792053\nstep: 100, loss: 0.10652050375938416\nstep: 110, loss: 0.1839829832315445\nstep: 120, loss: 0.012136949226260185\nstep: 130, loss: 0.29857006669044495\nstep: 140, loss: 0.1762515753507614\nstep: 150, loss: 0.058986105024814606\nstep: 160, loss: 0.06211082264780998\nstep: 170, loss: 0.12420687824487686\nstep: 180, loss: 0.028535468503832817\nstep: 190, loss: 0.008179369382560253\nstep: 200, loss: 0.013573364354670048\nstep: 210, loss: 0.024053938686847687\nstep: 220, loss: 0.034502144902944565\nstep: 230, loss: 0.06367278099060059\nstep: 240, loss: 0.21467258036136627\nstep: 250, loss: 0.08577010780572891\nstep: 260, loss: 0.008570174686610699\nstep: 270, loss: 0.08120334893465042\nstep: 280, loss: 0.10744338482618332\nstep: 290, loss: 0.10832788795232773\nstep: 300, loss: 0.01464166957885027\nstep: 310, loss: 0.05192391201853752\nstep: 320, loss: 0.10727204382419586\nstep: 330, loss: 0.2585950493812561\nstep: 340, loss: 0.1074361801147461\nstep: 350, loss: 0.020488502457737923\nstep: 360, loss: 0.030556097626686096\nstep: 370, loss: 0.15614111721515656\n0.8435885392202912\nstep: 0, loss: 0.14093157649040222\nstep: 10, loss: 0.01565822958946228\nstep: 20, loss: 0.028882447630167007\nstep: 30, loss: 0.1125238686800003\nstep: 40, loss: 0.13837049901485443\nstep: 50, loss: 0.24608498811721802\nstep: 60, loss: 0.01798132061958313\nstep: 70, loss: 0.014641784131526947\nstep: 80, loss: 0.06093492731451988\nstep: 90, loss: 0.005273299757391214\nstep: 100, loss: 0.011097951792180538\nstep: 110, loss: 0.09043101221323013\nstep: 120, loss: 0.3944319486618042\nstep: 130, loss: 0.030755698680877686\nstep: 140, loss: 0.3241940140724182\nstep: 150, loss: 0.5364803075790405\nstep: 160, loss: 0.19004541635513306\nstep: 170, loss: 0.6987596750259399\nstep: 180, loss: 0.07210687547922134\nstep: 190, loss: 0.18917390704154968\nstep: 200, loss: 0.06728975474834442\nstep: 210, loss: 0.39718255400657654\nstep: 220, loss: 0.19189821183681488\nstep: 230, loss: 0.0036545242182910442\nstep: 240, loss: 0.007899640128016472\nstep: 250, loss: 0.0041829501278698444\nstep: 260, loss: 0.399220734834671\nstep: 270, loss: 0.14148032665252686\nstep: 280, loss: 0.07031553983688354\nstep: 290, loss: 0.09218184649944305\nstep: 300, loss: 0.4720818102359772\nstep: 310, loss: 0.12146831303834915\nstep: 320, loss: 0.30795344710350037\nstep: 330, loss: 0.10914628952741623\nstep: 340, loss: 0.22414106130599976\nstep: 350, loss: 0.3318937122821808\nstep: 360, loss: 0.1156984344124794\nstep: 370, loss: 0.036932531744241714\n0.8233912635039925\nstep: 0, loss: 0.028612371534109116\nstep: 10, loss: 0.1789647489786148\nstep: 20, loss: 0.15367668867111206\nstep: 30, loss: 0.16903632879257202\nstep: 40, loss: 0.19834502041339874\nstep: 50, loss: 0.14494873583316803\nstep: 60, loss: 0.011427588760852814\nstep: 70, loss: 0.04688002169132233\nstep: 80, loss: 0.16347306966781616\nstep: 90, loss: 0.42252597212791443\nstep: 100, loss: 0.18620340526103973\nstep: 110, loss: 0.09033280611038208\nstep: 120, loss: 0.44769173860549927\nstep: 130, loss: 0.07813743501901627\nstep: 140, loss: 0.05487896502017975\nstep: 150, loss: 0.09312612563371658\nstep: 160, loss: 0.14786987006664276\nstep: 170, loss: 0.06348856538534164\nstep: 180, loss: 0.2502323389053345\nstep: 190, loss: 0.15561285614967346\nstep: 200, loss: 0.03537440672516823\nstep: 210, loss: 0.09182827919721603\nstep: 220, loss: 0.0963146761059761\nstep: 230, loss: 0.36286187171936035\nstep: 240, loss: 0.0027823662385344505\nstep: 250, loss: 1.488345980644226\nstep: 260, loss: 0.03420994430780411\nstep: 270, loss: 0.009411484003067017\nstep: 280, loss: 0.011799010448157787\nstep: 290, loss: 0.03676048293709755\nstep: 300, loss: 0.014492693357169628\nstep: 310, loss: 0.16618913412094116\nstep: 320, loss: 0.22813163697719574\nstep: 330, loss: 0.06416483223438263\nstep: 340, loss: 0.022077353671193123\nstep: 350, loss: 0.012837076559662819\nstep: 360, loss: 0.01597451977431774\nstep: 370, loss: 0.07000914216041565\n0.8633161108501644\nstep: 0, loss: 0.007500254083424807\nstep: 10, loss: 0.12503869831562042\nstep: 20, loss: 0.002193441381677985\nstep: 30, loss: 0.14893297851085663\nstep: 40, loss: 0.23028747737407684\nstep: 50, loss: 0.126815527677536\nstep: 60, loss: 0.12227298319339752\nstep: 70, loss: 0.2113240659236908\nstep: 80, loss: 0.07934165745973587\nstep: 90, loss: 0.18726982176303864\nstep: 100, loss: 0.14026184380054474\nstep: 110, loss: 0.20608004927635193\nstep: 120, loss: 0.14939649403095245\nstep: 130, loss: 0.31554749608039856\nstep: 140, loss: 0.7094830870628357\nstep: 150, loss: 0.13091953098773956\nstep: 160, loss: 0.04310844466090202\nstep: 170, loss: 0.1401127278804779\nstep: 180, loss: 0.19505907595157623\nstep: 190, loss: 0.2968917787075043\nstep: 200, loss: 0.16740643978118896\nstep: 210, loss: 0.08822362869977951\nstep: 220, loss: 0.5636577010154724\nstep: 230, loss: 0.4022296965122223\nstep: 240, loss: 0.2736803889274597\nstep: 250, loss: 0.05842973664402962\nstep: 260, loss: 0.04828471317887306\nstep: 270, loss: 0.010065941140055656\nstep: 280, loss: 0.2918611764907837\nstep: 290, loss: 0.008890615776181221\nstep: 300, loss: 0.24943900108337402\nstep: 310, loss: 0.012209395878016949\nstep: 320, loss: 0.00389240519143641\nstep: 330, loss: 0.26849764585494995\nstep: 340, loss: 0.27607816457748413\nstep: 350, loss: 0.026208950206637383\nstep: 360, loss: 0.02882317453622818\nstep: 370, loss: 0.021752823144197464\n0.8576796618130578\nstep: 0, loss: 0.06439334154129028\nstep: 10, loss: 0.002627677982673049\nstep: 20, loss: 0.31037071347236633\nstep: 30, loss: 0.334888219833374\nstep: 40, loss: 0.18674632906913757\nstep: 50, loss: 0.4950902760028839\nstep: 60, loss: 0.220603808760643\nstep: 70, loss: 0.3010166883468628\nstep: 80, loss: 0.2108185589313507\nstep: 90, loss: 0.020496968179941177\nstep: 100, loss: 0.4191833436489105\nstep: 110, loss: 0.1977323442697525\nstep: 120, loss: 0.1074594035744667\nstep: 130, loss: 0.21091656386852264\nstep: 140, loss: 0.20307953655719757\nstep: 150, loss: 0.15275920927524567\nstep: 160, loss: 0.13172799348831177\nstep: 170, loss: 0.3732857406139374\nstep: 180, loss: 0.028023499995470047\nstep: 190, loss: 0.012848380953073502\nstep: 200, loss: 0.03474375233054161\nstep: 210, loss: 0.029371649026870728\nstep: 220, loss: 0.23024584352970123\nstep: 230, loss: 0.18153998255729675\nstep: 240, loss: 0.006371222902089357\nstep: 250, loss: 0.008747612126171589\nstep: 260, loss: 0.15112167596817017\nstep: 270, loss: 0.034677598625421524\nstep: 280, loss: 0.019895821809768677\nstep: 290, loss: 0.3463376760482788\nstep: 300, loss: 0.15801577270030975\nstep: 310, loss: 0.2253098338842392\nstep: 320, loss: 0.1885656714439392\nstep: 330, loss: 0.009701584465801716\nstep: 340, loss: 0.11368197947740555\nstep: 350, loss: 0.05617465078830719\nstep: 360, loss: 0.010638908483088017\nstep: 370, loss: 0.289519339799881\n0.8421794269610146\nstep: 0, loss: 0.28650739789009094\nstep: 10, loss: 0.25095683336257935\nstep: 20, loss: 0.23638157546520233\nstep: 30, loss: 0.10973532497882843\nstep: 40, loss: 0.6332687139511108\nstep: 50, loss: 0.010104051791131496\nstep: 60, loss: 0.14407464861869812\nstep: 70, loss: 0.31740814447402954\nstep: 80, loss: 0.16450241208076477\nstep: 90, loss: 0.04743071645498276\nstep: 100, loss: 0.022926028817892075\nstep: 110, loss: 0.1797018051147461\nstep: 120, loss: 0.04529516026377678\nstep: 130, loss: 0.039747145026922226\nstep: 140, loss: 0.4128396511077881\nstep: 150, loss: 0.009431998245418072\nstep: 160, loss: 0.00894420687109232\nstep: 170, loss: 0.019968722015619278\nstep: 180, loss: 0.006586673203855753\nstep: 190, loss: 0.005231517367064953\nstep: 200, loss: 0.07720636576414108\nstep: 210, loss: 0.06201062723994255\nstep: 220, loss: 0.11175525188446045\nstep: 230, loss: 0.5073886513710022\nstep: 240, loss: 0.014042983762919903\nstep: 250, loss: 0.0035321384202688932\nstep: 260, loss: 0.3564809560775757\nstep: 270, loss: 0.3592297434806824\nstep: 280, loss: 0.2053934931755066\nstep: 290, loss: 0.26488423347473145\nstep: 300, loss: 0.06912356615066528\nstep: 310, loss: 0.0170054342597723\nstep: 320, loss: 0.10075408965349197\nstep: 330, loss: 0.025885730981826782\nstep: 340, loss: 0.05866425856947899\nstep: 350, loss: 0.03656308725476265\nstep: 360, loss: 0.17925439774990082\nstep: 370, loss: 0.03167971223592758\n0.8656646312822922\n[0.769375293565054, 0.8191639267261626, 0.8642555190230155, 0.8656646312822922, 0.8435885392202912, 0.8233912635039925, 0.8633161108501644, 0.8576796618130578, 0.8421794269610146, 0.8656646312822922]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_swwo = Net(modeltype='Davlan/bert-base-multilingual-cased-finetuned-swahili',vocab_size=len(tag2idx))\nmodel_swwo.to(device)\nmodel_swwo = nn.DataParallel(model_swwo)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:45:14.670584Z","iopub.execute_input":"2022-04-19T17:45:14.671192Z","iopub.status.idle":"2022-04-19T17:45:17.528788Z","shell.execute_reply.started":"2022-04-19T17:45:14.671153Z","shell.execute_reply":"2022-04-19T17:45:17.527854Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-swahili were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-swahili and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"Epoch=10\nswwo_acc=[]\nswwo_train=[]\ni=0\nwhile i<10: \n    swwo_train.append(train(model_swwo, wo_train_iter))\n    swwo_acc.append(eval(model_swwo, wo_test_iter))\n    i+=1\nprint(swwo_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:45:18.891075Z","iopub.execute_input":"2022-04-19T17:45:18.891331Z","iopub.status.idle":"2022-04-19T17:48:57.535954Z","shell.execute_reply.started":"2022-04-19T17:45:18.891303Z","shell.execute_reply":"2022-04-19T17:48:57.535122Z"},"trusted":true},"execution_count":179,"outputs":[{"name":"stdout","text":"step: 0, loss: 2.7803096771240234\nstep: 10, loss: 2.1786231994628906\nstep: 20, loss: 1.1785383224487305\nstep: 30, loss: 1.0362190008163452\nstep: 40, loss: 0.7631415128707886\nstep: 50, loss: 0.45705127716064453\nstep: 60, loss: 0.5076994299888611\nstep: 70, loss: 1.0313832759857178\nstep: 80, loss: 0.8747393488883972\nstep: 90, loss: 0.6641660332679749\nstep: 100, loss: 0.5760027766227722\nstep: 110, loss: 0.656756579875946\nstep: 120, loss: 0.4763466715812683\nstep: 130, loss: 0.8955742120742798\nstep: 140, loss: 0.596367359161377\nstep: 150, loss: 1.0544548034667969\nstep: 160, loss: 0.7169480323791504\nstep: 170, loss: 0.8767340779304504\nstep: 180, loss: 0.48817846179008484\nstep: 190, loss: 0.7391293048858643\nstep: 200, loss: 0.44296014308929443\nstep: 210, loss: 0.2548103630542755\nstep: 220, loss: 0.684266209602356\nstep: 230, loss: 0.39336150884628296\nstep: 240, loss: 0.37028002738952637\nstep: 250, loss: 0.23500105738639832\nstep: 260, loss: 0.4098683297634125\nstep: 270, loss: 0.34068217873573303\nstep: 280, loss: 0.472147136926651\nstep: 290, loss: 0.2336263209581375\nstep: 300, loss: 0.29094740748405457\nstep: 310, loss: 0.4015518128871918\nstep: 320, loss: 0.37636226415634155\nstep: 330, loss: 0.330975741147995\nstep: 340, loss: 0.6356200575828552\nstep: 350, loss: 0.45800861716270447\nstep: 360, loss: 0.928446888923645\nstep: 370, loss: 0.26330000162124634\n0.8121183654297792\nstep: 0, loss: 0.2951219975948334\nstep: 10, loss: 0.2612561881542206\nstep: 20, loss: 0.4574512541294098\nstep: 30, loss: 0.23885338008403778\nstep: 40, loss: 0.2689395546913147\nstep: 50, loss: 0.33546578884124756\nstep: 60, loss: 0.2710931897163391\nstep: 70, loss: 0.906144917011261\nstep: 80, loss: 0.170353963971138\nstep: 90, loss: 0.22371286153793335\nstep: 100, loss: 0.3000982701778412\nstep: 110, loss: 0.19239899516105652\nstep: 120, loss: 0.06284786760807037\nstep: 130, loss: 0.1692683845758438\nstep: 140, loss: 0.09943944960832596\nstep: 150, loss: 0.2562311887741089\nstep: 160, loss: 0.2827751636505127\nstep: 170, loss: 0.30023276805877686\nstep: 180, loss: 0.3845266103744507\nstep: 190, loss: 0.22130917012691498\nstep: 200, loss: 1.431633710861206\nstep: 210, loss: 0.18416322767734528\nstep: 220, loss: 2.758395195007324\nstep: 230, loss: 0.3461741507053375\nstep: 240, loss: 0.3214840292930603\nstep: 250, loss: 0.14778535068035126\nstep: 260, loss: 0.06895237416028976\nstep: 270, loss: 0.5370663404464722\nstep: 280, loss: 0.5114791393280029\nstep: 290, loss: 0.30697739124298096\nstep: 300, loss: 0.3495451807975769\nstep: 310, loss: 0.17808285355567932\nstep: 320, loss: 0.12540210783481598\nstep: 330, loss: 0.08858675509691238\nstep: 340, loss: 0.45959940552711487\nstep: 350, loss: 0.2447558045387268\nstep: 360, loss: 0.08830545842647552\nstep: 370, loss: 0.2876153290271759\n0.8473461719116956\nstep: 0, loss: 0.3996048867702484\nstep: 10, loss: 0.20387887954711914\nstep: 20, loss: 0.03919488564133644\nstep: 30, loss: 0.026278316974639893\nstep: 40, loss: 0.2890445291996002\nstep: 50, loss: 0.18129821121692657\nstep: 60, loss: 0.06417999416589737\nstep: 70, loss: 0.12364029884338379\nstep: 80, loss: 0.13000044226646423\nstep: 90, loss: 0.27161726355552673\nstep: 100, loss: 0.059920597821474075\nstep: 110, loss: 0.08547110855579376\nstep: 120, loss: 0.49696603417396545\nstep: 130, loss: 0.14778907597064972\nstep: 140, loss: 0.29567933082580566\nstep: 150, loss: 0.25546327233314514\nstep: 160, loss: 0.17150282859802246\nstep: 170, loss: 0.3909807801246643\nstep: 180, loss: 0.20744095742702484\nstep: 190, loss: 0.027953583747148514\nstep: 200, loss: 0.05700790137052536\nstep: 210, loss: 0.3814631700515747\nstep: 220, loss: 0.08650504797697067\nstep: 230, loss: 0.289209246635437\nstep: 240, loss: 0.030098600313067436\nstep: 250, loss: 0.08546856045722961\nstep: 260, loss: 0.5232341885566711\nstep: 270, loss: 0.1011764258146286\nstep: 280, loss: 0.06463570892810822\nstep: 290, loss: 0.19012562930583954\nstep: 300, loss: 0.16576522588729858\nstep: 310, loss: 0.2737038731575012\nstep: 320, loss: 0.2626495957374573\nstep: 330, loss: 0.02350509911775589\nstep: 340, loss: 0.01845509558916092\nstep: 350, loss: 0.2438715398311615\nstep: 360, loss: 0.04303901642560959\nstep: 370, loss: 0.11731179058551788\n0.8741193048379521\nstep: 0, loss: 0.07730905711650848\nstep: 10, loss: 0.24940018355846405\nstep: 20, loss: 0.077652208507061\nstep: 30, loss: 0.5105316042900085\nstep: 40, loss: 0.007904911413788795\nstep: 50, loss: 0.005031752400100231\nstep: 60, loss: 0.09806729853153229\nstep: 70, loss: 0.24586009979248047\nstep: 80, loss: 0.3840797245502472\nstep: 90, loss: 0.08162254840135574\nstep: 100, loss: 0.20921094715595245\nstep: 110, loss: 0.044786397367715836\nstep: 120, loss: 0.2520838677883148\nstep: 130, loss: 0.15324759483337402\nstep: 140, loss: 0.28677380084991455\nstep: 150, loss: 0.01321178674697876\nstep: 160, loss: 0.24835553765296936\nstep: 170, loss: 0.5571117401123047\nstep: 180, loss: 0.28054237365722656\nstep: 190, loss: 0.10271721333265305\nstep: 200, loss: 0.013368424028158188\nstep: 210, loss: 0.24540002644062042\nstep: 220, loss: 0.2278311401605606\nstep: 230, loss: 0.11730620265007019\nstep: 240, loss: 0.2436390221118927\nstep: 250, loss: 0.25856852531433105\nstep: 260, loss: 0.012711136601865292\nstep: 270, loss: 0.02691454440355301\nstep: 280, loss: 0.13424110412597656\nstep: 290, loss: 0.15628860890865326\nstep: 300, loss: 0.3250974416732788\nstep: 310, loss: 0.26342353224754333\nstep: 320, loss: 0.024516617879271507\nstep: 330, loss: 0.44065335392951965\nstep: 340, loss: 0.1509164273738861\nstep: 350, loss: 0.06669606268405914\nstep: 360, loss: 0.171810582280159\nstep: 370, loss: 0.22780923545360565\n0.8543917332080789\nstep: 0, loss: 0.30140477418899536\nstep: 10, loss: 0.022582050412893295\nstep: 20, loss: 0.05229324474930763\nstep: 30, loss: 0.004776237532496452\nstep: 40, loss: 0.20590412616729736\nstep: 50, loss: 0.5913107395172119\nstep: 60, loss: 0.08999708294868469\nstep: 70, loss: 0.020510127767920494\nstep: 80, loss: 0.2536068260669708\nstep: 90, loss: 0.15051151812076569\nstep: 100, loss: 0.007445191033184528\nstep: 110, loss: 0.004671597853302956\nstep: 120, loss: 0.07018732279539108\nstep: 130, loss: 0.0028387426864355803\nstep: 140, loss: 0.06941670924425125\nstep: 150, loss: 0.223531112074852\nstep: 160, loss: 0.2912442088127136\nstep: 170, loss: 0.18991678953170776\nstep: 180, loss: 0.03284564986824989\nstep: 190, loss: 0.2410040646791458\nstep: 200, loss: 0.09625324606895447\nstep: 210, loss: 0.09385940432548523\nstep: 220, loss: 0.05678188055753708\nstep: 230, loss: 0.06421317160129547\nstep: 240, loss: 0.273666113615036\nstep: 250, loss: 0.049609292298555374\nstep: 260, loss: 0.07053124904632568\nstep: 270, loss: 0.1246512159705162\nstep: 280, loss: 0.1340944468975067\nstep: 290, loss: 0.1507556289434433\nstep: 300, loss: 0.1536952406167984\nstep: 310, loss: 0.008275493048131466\nstep: 320, loss: 0.027398666366934776\nstep: 330, loss: 0.05639411509037018\nstep: 340, loss: 0.16634640097618103\nstep: 350, loss: 0.30779147148132324\nstep: 360, loss: 0.3160860240459442\nstep: 370, loss: 0.1478031575679779\n0.8684828558008455\nstep: 0, loss: 0.4344036281108856\nstep: 10, loss: 0.12885615229606628\nstep: 20, loss: 0.04463627561926842\nstep: 30, loss: 0.04433596506714821\nstep: 40, loss: 0.005790308117866516\nstep: 50, loss: 0.005571219138801098\nstep: 60, loss: 0.26948365569114685\nstep: 70, loss: 0.06003427132964134\nstep: 80, loss: 0.13487392663955688\nstep: 90, loss: 0.3984556198120117\nstep: 100, loss: 0.03688646852970123\nstep: 110, loss: 0.26622727513313293\nstep: 120, loss: 0.007840119302272797\nstep: 130, loss: 0.15973049402236938\nstep: 140, loss: 1.0090208053588867\nstep: 150, loss: 0.015834158286452293\nstep: 160, loss: 0.14061102271080017\nstep: 170, loss: 0.072393037378788\nstep: 180, loss: 0.013554246164858341\nstep: 190, loss: 0.019472889602184296\nstep: 200, loss: 0.26886600255966187\nstep: 210, loss: 0.28483498096466064\nstep: 220, loss: 0.2530333995819092\nstep: 230, loss: 0.20451487600803375\nstep: 240, loss: 0.07418479025363922\nstep: 250, loss: 0.034807536751031876\nstep: 260, loss: 0.3337380588054657\nstep: 270, loss: 0.17743682861328125\nstep: 280, loss: 0.08297928422689438\nstep: 290, loss: 0.012650121934711933\nstep: 300, loss: 0.27152594923973083\nstep: 310, loss: 0.38402628898620605\nstep: 320, loss: 0.14825424551963806\nstep: 330, loss: 0.011548985727131367\nstep: 340, loss: 0.16853787004947662\nstep: 350, loss: 0.128811776638031\nstep: 360, loss: 0.4482842981815338\nstep: 370, loss: 0.3437804877758026\n0.8111789572569281\nstep: 0, loss: 0.1830470860004425\nstep: 10, loss: 0.486242413520813\nstep: 20, loss: 0.3191165626049042\nstep: 30, loss: 0.3610052764415741\nstep: 40, loss: 0.2549888789653778\nstep: 50, loss: 0.07829498499631882\nstep: 60, loss: 0.8147302865982056\nstep: 70, loss: 0.4245784282684326\nstep: 80, loss: 0.3494822680950165\nstep: 90, loss: 0.18562358617782593\nstep: 100, loss: 0.14812633395195007\nstep: 110, loss: 0.03267033025622368\nstep: 120, loss: 0.0488181933760643\nstep: 130, loss: 0.05587106570601463\nstep: 140, loss: 0.4940434396266937\nstep: 150, loss: 0.5773980617523193\nstep: 160, loss: 0.10731054097414017\nstep: 170, loss: 0.10904520004987717\nstep: 180, loss: 0.1264113187789917\nstep: 190, loss: 0.021308809518814087\nstep: 200, loss: 0.2806842625141144\nstep: 210, loss: 0.32236137986183167\nstep: 220, loss: 0.7326486706733704\nstep: 230, loss: 0.14434769749641418\nstep: 240, loss: 0.24564974009990692\nstep: 250, loss: 0.38234835863113403\nstep: 260, loss: 0.12578178942203522\nstep: 270, loss: 0.04274821653962135\nstep: 280, loss: 0.01389940083026886\nstep: 290, loss: 0.09075487405061722\nstep: 300, loss: 0.40877026319503784\nstep: 310, loss: 0.11246372759342194\nstep: 320, loss: 0.07987876236438751\nstep: 330, loss: 0.2108333557844162\nstep: 340, loss: 0.2224506139755249\nstep: 350, loss: 0.18345798552036285\nstep: 360, loss: 0.2063542753458023\nstep: 370, loss: 0.028525199741125107\n0.8661343353687178\nstep: 0, loss: 0.11657766997814178\nstep: 10, loss: 0.41568523645401\nstep: 20, loss: 0.01909616030752659\nstep: 30, loss: 0.5353490114212036\nstep: 40, loss: 0.015828954055905342\nstep: 50, loss: 0.011158391833305359\nstep: 60, loss: 0.1123957559466362\nstep: 70, loss: 0.12131965905427933\nstep: 80, loss: 0.025006521493196487\nstep: 90, loss: 0.02544993720948696\nstep: 100, loss: 0.0905018299818039\nstep: 110, loss: 0.10240150988101959\nstep: 120, loss: 0.12919099628925323\nstep: 130, loss: 0.15549194812774658\nstep: 140, loss: 0.13488736748695374\nstep: 150, loss: 0.004225583281368017\nstep: 160, loss: 0.00606535142287612\nstep: 170, loss: 0.24439957737922668\nstep: 180, loss: 0.061382390558719635\nstep: 190, loss: 0.29627594351768494\nstep: 200, loss: 0.3906993269920349\nstep: 210, loss: 0.029196497052907944\nstep: 220, loss: 0.07492645829916\nstep: 230, loss: 0.22825054824352264\nstep: 240, loss: 0.09795026481151581\nstep: 250, loss: 0.02065904252231121\nstep: 260, loss: 0.03747865557670593\nstep: 270, loss: 0.08669505268335342\nstep: 280, loss: 0.02250763773918152\nstep: 290, loss: 0.359883189201355\nstep: 300, loss: 0.37688615918159485\nstep: 310, loss: 0.26519036293029785\nstep: 320, loss: 0.20068231225013733\nstep: 330, loss: 0.16375736892223358\nstep: 340, loss: 0.12491876631975174\nstep: 350, loss: 0.14985314011573792\nstep: 360, loss: 0.25119689106941223\nstep: 370, loss: 0.16828523576259613\n0.8487552841709722\nstep: 0, loss: 0.2102293074131012\nstep: 10, loss: 0.33107784390449524\nstep: 20, loss: 0.11300305277109146\nstep: 30, loss: 0.33578208088874817\nstep: 40, loss: 0.1561410278081894\nstep: 50, loss: 0.03335260599851608\nstep: 60, loss: 0.058548424392938614\nstep: 70, loss: 0.01286329422146082\nstep: 80, loss: 0.016432859003543854\nstep: 90, loss: 0.1971912533044815\nstep: 100, loss: 0.25144335627555847\nstep: 110, loss: 0.08147381246089935\nstep: 120, loss: 0.2776292860507965\nstep: 130, loss: 0.14083270728588104\nstep: 140, loss: 0.20374834537506104\nstep: 150, loss: 0.04507554695010185\nstep: 160, loss: 0.3655998110771179\nstep: 170, loss: 0.4971945583820343\nstep: 180, loss: 0.8402628302574158\nstep: 190, loss: 0.6240768432617188\nstep: 200, loss: 0.02337370440363884\nstep: 210, loss: 0.21220830082893372\nstep: 220, loss: 0.19875140488147736\nstep: 230, loss: 0.3734738826751709\nstep: 240, loss: 0.3560822010040283\nstep: 250, loss: 0.3105747401714325\nstep: 260, loss: 0.33827316761016846\nstep: 270, loss: 0.18896302580833435\nstep: 280, loss: 0.22208689153194427\nstep: 290, loss: 0.17339275777339935\nstep: 300, loss: 0.045672457665205\nstep: 310, loss: 0.2669917047023773\nstep: 320, loss: 0.12367955595254898\nstep: 330, loss: 0.09676031023263931\nstep: 340, loss: 0.5998281240463257\nstep: 350, loss: 0.11329241842031479\nstep: 360, loss: 0.030266324058175087\nstep: 370, loss: 0.10534218698740005\n0.8027242837012682\nstep: 0, loss: 0.401220440864563\nstep: 10, loss: 0.03298138827085495\nstep: 20, loss: 0.38119086623191833\nstep: 30, loss: 0.16867204010486603\nstep: 40, loss: 0.375419020652771\nstep: 50, loss: 0.09693269431591034\nstep: 60, loss: 0.010513081215322018\nstep: 70, loss: 0.28398996591567993\nstep: 80, loss: 0.012466561049222946\nstep: 90, loss: 0.02809845469892025\nstep: 100, loss: 0.05151096358895302\nstep: 110, loss: 0.19085581600666046\nstep: 120, loss: 0.32919344305992126\nstep: 130, loss: 0.32012060284614563\nstep: 140, loss: 0.23839432001113892\nstep: 150, loss: 0.3987436592578888\nstep: 160, loss: 0.5084429383277893\nstep: 170, loss: 0.9839237928390503\nstep: 180, loss: 0.16793286800384521\nstep: 190, loss: 0.36077263951301575\nstep: 200, loss: 0.18828760087490082\nstep: 210, loss: 0.3516003489494324\nstep: 220, loss: 0.43932488560676575\nstep: 230, loss: 0.12763026356697083\nstep: 240, loss: 0.3326661288738251\nstep: 250, loss: 0.09842557460069656\nstep: 260, loss: 0.2221531867980957\nstep: 270, loss: 0.06694066524505615\nstep: 280, loss: 0.14637483656406403\nstep: 290, loss: 0.24435369670391083\nstep: 300, loss: 0.20444853603839874\nstep: 310, loss: 0.06030334159731865\nstep: 320, loss: 0.03503302484750748\nstep: 330, loss: 0.22332064807415009\nstep: 340, loss: 0.03182539343833923\nstep: 350, loss: 0.7004972100257874\nstep: 360, loss: 0.07377809286117554\nstep: 370, loss: 0.2802026569843292\n0.868952559887271\n[0.8121183654297792, 0.8473461719116956, 0.8741193048379521, 0.8543917332080789, 0.8684828558008455, 0.8111789572569281, 0.8661343353687178, 0.8487552841709722, 0.8027242837012682, 0.868952559887271]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_amwo = Net(modeltype='Davlan/bert-base-multilingual-cased-finetuned-amharic',vocab_size=len(tag2idx))\nmodel_amwo.to(device)\nmodel_amwo = nn.DataParallel(model_amwo)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:51:07.616926Z","iopub.execute_input":"2022-04-19T17:51:07.617193Z","iopub.status.idle":"2022-04-19T17:53:22.178181Z","shell.execute_reply.started":"2022-04-19T17:51:07.617164Z","shell.execute_reply":"2022-04-19T17:53:22.177400Z"},"trusted":true},"execution_count":187,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/798 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9086fbc2dfea4b69aa567dd2eb024553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/679M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90769a0ba0244037a5d8c918dc8bd9a3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-amharic were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertModel were not initialized from the model checkpoint at Davlan/bert-base-multilingual-cased-finetuned-amharic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"Epoch=10\namwo_acc=[]\namwo_train=[]\ni=0\nwhile i<10: \n    amwo_train.append(train(model_amwo, wo_train_iter))\n    amwo_acc.append(eval(model_amwo, wo_test_iter))\n    i+=1\nprint(amwo_acc)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:53:32.774997Z","iopub.execute_input":"2022-04-19T17:53:32.775251Z","iopub.status.idle":"2022-04-19T17:57:06.818609Z","shell.execute_reply.started":"2022-04-19T17:53:32.775224Z","shell.execute_reply":"2022-04-19T17:57:06.817761Z"},"trusted":true},"execution_count":188,"outputs":[{"name":"stdout","text":"step: 0, loss: 2.8901472091674805\nstep: 10, loss: 1.8946261405944824\nstep: 20, loss: 1.8354579210281372\nstep: 30, loss: 0.7826298475265503\nstep: 40, loss: 0.9708350896835327\nstep: 50, loss: 0.8001859784126282\nstep: 60, loss: 0.7295908331871033\nstep: 70, loss: 0.8069336414337158\nstep: 80, loss: 0.5777379274368286\nstep: 90, loss: 0.7464367747306824\nstep: 100, loss: 0.5633741617202759\nstep: 110, loss: 0.5333417057991028\nstep: 120, loss: 0.3086574971675873\nstep: 130, loss: 0.6080670952796936\nstep: 140, loss: 0.6079689264297485\nstep: 150, loss: 0.8216054439544678\nstep: 160, loss: 0.45692211389541626\nstep: 170, loss: 0.7675144076347351\nstep: 180, loss: 0.6642245650291443\nstep: 190, loss: 0.44635263085365295\nstep: 200, loss: 0.2988959550857544\nstep: 210, loss: 0.37927037477493286\nstep: 220, loss: 0.5609097480773926\nstep: 230, loss: 0.9226611852645874\nstep: 240, loss: 0.4876823425292969\nstep: 250, loss: 0.5849490165710449\nstep: 260, loss: 0.360556423664093\nstep: 270, loss: 0.4446164667606354\nstep: 280, loss: 0.9009416103363037\nstep: 290, loss: 0.44064921140670776\nstep: 300, loss: 0.49451345205307007\nstep: 310, loss: 0.3813103139400482\nstep: 320, loss: 0.30821725726127625\nstep: 330, loss: 0.4010002017021179\nstep: 340, loss: 0.22584286332130432\nstep: 350, loss: 0.2439708560705185\nstep: 360, loss: 0.2953527569770813\nstep: 370, loss: 0.763297438621521\n0.7858149365899484\nstep: 0, loss: 0.2169119417667389\nstep: 10, loss: 0.37522318959236145\nstep: 20, loss: 0.19747550785541534\nstep: 30, loss: 0.4121292233467102\nstep: 40, loss: 0.23646162450313568\nstep: 50, loss: 0.449640691280365\nstep: 60, loss: 0.0765174850821495\nstep: 70, loss: 0.24069693684577942\nstep: 80, loss: 0.20823253691196442\nstep: 90, loss: 0.0426868237555027\nstep: 100, loss: 0.4650111794471741\nstep: 110, loss: 0.19523249566555023\nstep: 120, loss: 0.24668847024440765\nstep: 130, loss: 0.33286112546920776\nstep: 140, loss: 0.05306485667824745\nstep: 150, loss: 0.1514953225851059\nstep: 160, loss: 0.39591988921165466\nstep: 170, loss: 0.44063225388526917\nstep: 180, loss: 0.2438601851463318\nstep: 190, loss: 0.574343204498291\nstep: 200, loss: 0.7388249039649963\nstep: 210, loss: 0.3898439407348633\nstep: 220, loss: 0.6183584332466125\nstep: 230, loss: 0.2609466016292572\nstep: 240, loss: 0.04187605530023575\nstep: 250, loss: 0.10225725173950195\nstep: 260, loss: 0.22744323313236237\nstep: 270, loss: 0.23992973566055298\nstep: 280, loss: 0.2061450332403183\nstep: 290, loss: 0.6217963099479675\nstep: 300, loss: 0.5811891555786133\nstep: 310, loss: 0.3412025570869446\nstep: 320, loss: 0.1379852443933487\nstep: 330, loss: 0.631178617477417\nstep: 340, loss: 0.24890683591365814\nstep: 350, loss: 0.2924089729785919\nstep: 360, loss: 0.25139445066452026\nstep: 370, loss: 0.05857323482632637\n0.8135274776890559\nstep: 0, loss: 0.03832351416349411\nstep: 10, loss: 0.11781007796525955\nstep: 20, loss: 0.02734728530049324\nstep: 30, loss: 0.2995484173297882\nstep: 40, loss: 0.4132576584815979\nstep: 50, loss: 0.5168936252593994\nstep: 60, loss: 0.1096692681312561\nstep: 70, loss: 0.14884193241596222\nstep: 80, loss: 0.25133225321769714\nstep: 90, loss: 0.03125515952706337\nstep: 100, loss: 0.06390002369880676\nstep: 110, loss: 0.09105733036994934\nstep: 120, loss: 0.2745703458786011\nstep: 130, loss: 0.3870719373226166\nstep: 140, loss: 0.10747507959604263\nstep: 150, loss: 0.27101123332977295\nstep: 160, loss: 0.2521674633026123\nstep: 170, loss: 0.02560395747423172\nstep: 180, loss: 0.38964635133743286\nstep: 190, loss: 0.1019645407795906\nstep: 200, loss: 0.06058395281434059\nstep: 210, loss: 0.2036479264497757\nstep: 220, loss: 0.0550416000187397\nstep: 230, loss: 0.1738070398569107\nstep: 240, loss: 0.5689852833747864\nstep: 250, loss: 0.22772110998630524\nstep: 260, loss: 0.18459348380565643\nstep: 270, loss: 0.3802517354488373\nstep: 280, loss: 0.07444503158330917\nstep: 290, loss: 0.12612344324588776\nstep: 300, loss: 0.35112693905830383\nstep: 310, loss: 0.031713590025901794\nstep: 320, loss: 0.01844594068825245\nstep: 330, loss: 0.048985403031110764\nstep: 340, loss: 0.2845571041107178\nstep: 350, loss: 0.2434581071138382\nstep: 360, loss: 0.13477325439453125\nstep: 370, loss: 0.3627574145793915\n0.8487552841709722\nstep: 0, loss: 0.09819544106721878\nstep: 10, loss: 0.058362189680337906\nstep: 20, loss: 0.5500075817108154\nstep: 30, loss: 0.012088838964700699\nstep: 40, loss: 0.14739644527435303\nstep: 50, loss: 0.2787330746650696\nstep: 60, loss: 0.019702553749084473\nstep: 70, loss: 0.036992039531469345\nstep: 80, loss: 0.26830655336380005\nstep: 90, loss: 0.03467043861746788\nstep: 100, loss: 0.22882360219955444\nstep: 110, loss: 0.3551505506038666\nstep: 120, loss: 0.15263357758522034\nstep: 130, loss: 0.34584295749664307\nstep: 140, loss: 0.2800987958908081\nstep: 150, loss: 0.2909839451313019\nstep: 160, loss: 0.21083594858646393\nstep: 170, loss: 0.4927217364311218\nstep: 180, loss: 0.3367424011230469\nstep: 190, loss: 0.329942524433136\nstep: 200, loss: 0.11963678151369095\nstep: 210, loss: 0.617588996887207\nstep: 220, loss: 0.05686245486140251\nstep: 230, loss: 0.5227305889129639\nstep: 240, loss: 0.3551812171936035\nstep: 250, loss: 0.06001992151141167\nstep: 260, loss: 0.18782471120357513\nstep: 270, loss: 0.014150072820484638\nstep: 280, loss: 0.1707277148962021\nstep: 290, loss: 0.17523300647735596\nstep: 300, loss: 0.0989643856883049\nstep: 310, loss: 0.4746681749820709\nstep: 320, loss: 0.20702043175697327\nstep: 330, loss: 0.30752941966056824\nstep: 340, loss: 0.14638423919677734\nstep: 350, loss: 0.03435290977358818\nstep: 360, loss: 0.18422208726406097\nstep: 370, loss: 0.00810531247407198\n0.8614372945044622\nstep: 0, loss: 0.04704301431775093\nstep: 10, loss: 0.1986178308725357\nstep: 20, loss: 0.15778878331184387\nstep: 30, loss: 0.5777689814567566\nstep: 40, loss: 0.2109641283750534\nstep: 50, loss: 0.1584760546684265\nstep: 60, loss: 0.1285969614982605\nstep: 70, loss: 0.1847905069589615\nstep: 80, loss: 0.08702822029590607\nstep: 90, loss: 0.08614875376224518\nstep: 100, loss: 0.3577507734298706\nstep: 110, loss: 0.04627043008804321\nstep: 120, loss: 0.21746830642223358\nstep: 130, loss: 0.0040331012569367886\nstep: 140, loss: 0.03002212941646576\nstep: 150, loss: 0.015884501859545708\nstep: 160, loss: 0.18124130368232727\nstep: 170, loss: 0.4734356701374054\nstep: 180, loss: 0.16394846141338348\nstep: 190, loss: 0.2582324743270874\nstep: 200, loss: 0.005273299291729927\nstep: 210, loss: 0.06496173143386841\nstep: 220, loss: 0.06359857320785522\nstep: 230, loss: 0.10129479318857193\nstep: 240, loss: 0.048706863075494766\nstep: 250, loss: 0.01445702649652958\nstep: 260, loss: 0.07230275869369507\nstep: 270, loss: 0.06541585177183151\nstep: 280, loss: 0.2591942250728607\nstep: 290, loss: 0.14378097653388977\nstep: 300, loss: 0.012994813732802868\nstep: 310, loss: 0.0976676419377327\nstep: 320, loss: 0.5661489367485046\nstep: 330, loss: 0.1074817106127739\nstep: 340, loss: 0.08210953325033188\nstep: 350, loss: 0.008458736352622509\nstep: 360, loss: 0.11570625752210617\nstep: 370, loss: 0.4367672801017761\n0.8379520901831846\nstep: 0, loss: 0.1688416302204132\nstep: 10, loss: 0.3820365369319916\nstep: 20, loss: 0.25964605808258057\nstep: 30, loss: 0.25258955359458923\nstep: 40, loss: 0.21059323847293854\nstep: 50, loss: 0.04286441579461098\nstep: 60, loss: 0.3354797661304474\nstep: 70, loss: 0.2030600756406784\nstep: 80, loss: 0.3806859254837036\nstep: 90, loss: 0.03982735797762871\nstep: 100, loss: 0.29346638917922974\nstep: 110, loss: 0.3506690561771393\nstep: 120, loss: 0.04331251606345177\nstep: 130, loss: 0.046802882105112076\nstep: 140, loss: 0.09248796850442886\nstep: 150, loss: 0.0064810519106686115\nstep: 160, loss: 0.043050266802310944\nstep: 170, loss: 0.007270680740475655\nstep: 180, loss: 0.18972806632518768\nstep: 190, loss: 0.03326338902115822\nstep: 200, loss: 0.03376080095767975\nstep: 210, loss: 0.021917805075645447\nstep: 220, loss: 0.33183637261390686\nstep: 230, loss: 0.0682571604847908\nstep: 240, loss: 0.012773365713655949\nstep: 250, loss: 0.271392822265625\nstep: 260, loss: 0.005595133639872074\nstep: 270, loss: 0.07411766052246094\nstep: 280, loss: 0.096550352871418\nstep: 290, loss: 0.008153881877660751\nstep: 300, loss: 0.08022138476371765\nstep: 310, loss: 0.17529188096523285\nstep: 320, loss: 0.0420314259827137\nstep: 330, loss: 0.4550897777080536\nstep: 340, loss: 0.10971755534410477\nstep: 350, loss: 0.005383873824030161\nstep: 360, loss: 0.2718413174152374\nstep: 370, loss: 0.04396658390760422\n0.8637858149365899\nstep: 0, loss: 0.1016082838177681\nstep: 10, loss: 0.29908931255340576\nstep: 20, loss: 0.05196061357855797\nstep: 30, loss: 0.050429441034793854\nstep: 40, loss: 0.01856013759970665\nstep: 50, loss: 0.12760907411575317\nstep: 60, loss: 0.03843052685260773\nstep: 70, loss: 0.21679356694221497\nstep: 80, loss: 0.030680982396006584\nstep: 90, loss: 0.27367329597473145\nstep: 100, loss: 0.0776773989200592\nstep: 110, loss: 0.07742282003164291\nstep: 120, loss: 0.39320969581604004\nstep: 130, loss: 0.2030799239873886\nstep: 140, loss: 0.01642056368291378\nstep: 150, loss: 0.12127233296632767\nstep: 160, loss: 0.014459248632192612\nstep: 170, loss: 0.005450781434774399\nstep: 180, loss: 0.10354866832494736\nstep: 190, loss: 0.019573012366890907\nstep: 200, loss: 0.033704690635204315\nstep: 210, loss: 0.3464418947696686\nstep: 220, loss: 0.0213833786547184\nstep: 230, loss: 0.015352953225374222\nstep: 240, loss: 0.0062265475280582905\nstep: 250, loss: 0.10668999701738358\nstep: 260, loss: 0.07621326297521591\nstep: 270, loss: 0.029041016474366188\nstep: 280, loss: 0.008747806772589684\nstep: 290, loss: 0.023753700777888298\nstep: 300, loss: 0.0037928877864032984\nstep: 310, loss: 0.046601664274930954\nstep: 320, loss: 0.13607272505760193\nstep: 330, loss: 0.10929872840642929\nstep: 340, loss: 0.01822630874812603\nstep: 350, loss: 0.01749122515320778\nstep: 360, loss: 0.10876881331205368\nstep: 370, loss: 0.05819752812385559\n0.8449976514795678\nstep: 0, loss: 0.047195352613925934\nstep: 10, loss: 0.01747632585465908\nstep: 20, loss: 0.06921126693487167\nstep: 30, loss: 0.2670576870441437\nstep: 40, loss: 0.016719955950975418\nstep: 50, loss: 0.0777900442481041\nstep: 60, loss: 0.09161581099033356\nstep: 70, loss: 0.003481475170701742\nstep: 80, loss: 0.05782167240977287\nstep: 90, loss: 0.1343620866537094\nstep: 100, loss: 0.4060550928115845\nstep: 110, loss: 0.12260425835847855\nstep: 120, loss: 0.03360433131456375\nstep: 130, loss: 0.014072609134018421\nstep: 140, loss: 0.04128566384315491\nstep: 150, loss: 0.11735085397958755\nstep: 160, loss: 0.033625975251197815\nstep: 170, loss: 0.004568872042000294\nstep: 180, loss: 0.03246695548295975\nstep: 190, loss: 0.07212851196527481\nstep: 200, loss: 0.3149397373199463\nstep: 210, loss: 0.017641423270106316\nstep: 220, loss: 0.025528041645884514\nstep: 230, loss: 0.1838674545288086\nstep: 240, loss: 0.004800874274224043\nstep: 250, loss: 0.0358000211417675\nstep: 260, loss: 0.09461908787488937\nstep: 270, loss: 0.28982076048851013\nstep: 280, loss: 0.009321939200162888\nstep: 290, loss: 0.2629763185977936\nstep: 300, loss: 0.06071436032652855\nstep: 310, loss: 0.09482240676879883\nstep: 320, loss: 0.16878047585487366\nstep: 330, loss: 0.14571033418178558\nstep: 340, loss: 0.006216866429895163\nstep: 350, loss: 0.3294323980808258\nstep: 360, loss: 0.026719016954302788\nstep: 370, loss: 0.1758045107126236\n0.8543917332080789\nstep: 0, loss: 0.12227283418178558\nstep: 10, loss: 0.12207964807748795\nstep: 20, loss: 0.4080730080604553\nstep: 30, loss: 0.45540767908096313\nstep: 40, loss: 0.22774375975131989\nstep: 50, loss: 0.2936677634716034\nstep: 60, loss: 0.03580869734287262\nstep: 70, loss: 0.015353196300566196\nstep: 80, loss: 0.16585493087768555\nstep: 90, loss: 0.1204834133386612\nstep: 100, loss: 0.2757214903831482\nstep: 110, loss: 0.04630958288908005\nstep: 120, loss: 0.14779505133628845\nstep: 130, loss: 0.16820739209651947\nstep: 140, loss: 0.10526733100414276\nstep: 150, loss: 0.1304549127817154\nstep: 160, loss: 0.2611255943775177\nstep: 170, loss: 0.21635757386684418\nstep: 180, loss: 0.014016024768352509\nstep: 190, loss: 0.0028010879177600145\nstep: 200, loss: 0.06410635262727737\nstep: 210, loss: 0.2722281515598297\nstep: 220, loss: 0.007330755703151226\nstep: 230, loss: 0.008225194178521633\nstep: 240, loss: 0.15443627536296844\nstep: 250, loss: 0.05904766544699669\nstep: 260, loss: 0.39997902512550354\nstep: 270, loss: 0.4143812358379364\nstep: 280, loss: 0.05652843415737152\nstep: 290, loss: 0.028171900659799576\nstep: 300, loss: 0.09222930669784546\nstep: 310, loss: 0.2363467514514923\nstep: 320, loss: 0.024726424366235733\nstep: 330, loss: 0.23432646691799164\nstep: 340, loss: 0.018216928467154503\nstep: 350, loss: 0.029344776645302773\nstep: 360, loss: 0.12520283460617065\nstep: 370, loss: 0.11967363208532333\n0.8567402536402067\nstep: 0, loss: 0.0042369794100522995\nstep: 10, loss: 0.23024746775627136\nstep: 20, loss: 0.07165399938821793\nstep: 30, loss: 0.09779555350542068\nstep: 40, loss: 0.012871394865214825\nstep: 50, loss: 0.00427031796425581\nstep: 60, loss: 0.005163435824215412\nstep: 70, loss: 0.14265917241573334\nstep: 80, loss: 0.06366699934005737\nstep: 90, loss: 0.047881025820970535\nstep: 100, loss: 0.002395043382421136\nstep: 110, loss: 0.00692478334531188\nstep: 120, loss: 0.19300895929336548\nstep: 130, loss: 0.059694718569517136\nstep: 140, loss: 0.018509885296225548\nstep: 150, loss: 0.0029358048923313618\nstep: 160, loss: 0.0030766043346375227\nstep: 170, loss: 0.011294662021100521\nstep: 180, loss: 0.13573727011680603\nstep: 190, loss: 0.24799126386642456\nstep: 200, loss: 0.08088157325983047\nstep: 210, loss: 0.12481645494699478\nstep: 220, loss: 0.0573577880859375\nstep: 230, loss: 0.6013363003730774\nstep: 240, loss: 0.005063536111265421\nstep: 250, loss: 0.18370604515075684\nstep: 260, loss: 0.1552957445383072\nstep: 270, loss: 0.00501190684735775\nstep: 280, loss: 0.0174267515540123\nstep: 290, loss: 0.09484502673149109\nstep: 300, loss: 0.11443442851305008\nstep: 310, loss: 0.48363521695137024\nstep: 320, loss: 0.3395148813724518\nstep: 330, loss: 0.20547348260879517\nstep: 340, loss: 0.09888136386871338\nstep: 350, loss: 0.14755955338478088\nstep: 360, loss: 0.08953110128641129\nstep: 370, loss: 0.18683575093746185\n0.8356035697510569\n[0.7858149365899484, 0.8135274776890559, 0.8487552841709722, 0.8614372945044622, 0.8379520901831846, 0.8637858149365899, 0.8449976514795678, 0.8543917332080789, 0.8567402536402067, 0.8356035697510569]\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.plot(wo_acc)\nplt.plot(swwo_acc)\nplt.plot(yowo_acc)\nplt.plot(amwo_acc)\nplt.ylabel('F1')\nplt.xlabel('Epoch')\nplt.legend([\"original\", \"sw\",\"yo\",\"am\"], loc =\"lower right\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:01:03.656545Z","iopub.execute_input":"2022-04-19T18:01:03.656812Z","iopub.status.idle":"2022-04-19T18:01:03.840298Z","shell.execute_reply.started":"2022-04-19T18:01:03.656784Z","shell.execute_reply":"2022-04-19T18:01:03.839642Z"},"trusted":true},"execution_count":192,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"mean_yo=sum(yowo_acc)/10\nmean_yo","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:49:13.600812Z","iopub.execute_input":"2022-04-19T17:49:13.601550Z","iopub.status.idle":"2022-04-19T17:49:13.608363Z","shell.execute_reply.started":"2022-04-19T17:49:13.601512Z","shell.execute_reply":"2022-04-19T17:49:13.607493Z"},"trusted":true},"execution_count":183,"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"0.8414279004227337"},"metadata":{}}]},{"cell_type":"code","source":"mean_og=sum(wo_acc)/10\nmean_og","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:49:17.122704Z","iopub.execute_input":"2022-04-19T17:49:17.123245Z","iopub.status.idle":"2022-04-19T17:49:17.128899Z","shell.execute_reply.started":"2022-04-19T17:49:17.123208Z","shell.execute_reply":"2022-04-19T17:49:17.128184Z"},"trusted":true},"execution_count":184,"outputs":[{"execution_count":184,"output_type":"execute_result","data":{"text/plain":"0.8337247534053546"},"metadata":{}}]},{"cell_type":"code","source":"mean_sw=sum(swwo_acc)/10\nmean_sw","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:49:19.136836Z","iopub.execute_input":"2022-04-19T17:49:19.137611Z","iopub.status.idle":"2022-04-19T17:49:19.144111Z","shell.execute_reply.started":"2022-04-19T17:49:19.137547Z","shell.execute_reply":"2022-04-19T17:49:19.143105Z"},"trusted":true},"execution_count":185,"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"0.8454203851573509"},"metadata":{}}]},{"cell_type":"code","source":"mean_am=sum(amwo_acc)/10\nmean_am","metadata":{"execution":{"iopub.status.busy":"2022-04-19T17:57:44.513852Z","iopub.execute_input":"2022-04-19T17:57:44.514156Z","iopub.status.idle":"2022-04-19T17:57:44.521910Z","shell.execute_reply.started":"2022-04-19T17:57:44.514125Z","shell.execute_reply":"2022-04-19T17:57:44.521149Z"},"trusted":true},"execution_count":189,"outputs":[{"execution_count":189,"output_type":"execute_result","data":{"text/plain":"0.8403006106153125"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.DataFrame(\n    {\n        \"Original\":wo_acc,\n        \"Swahili\":swwo_acc,\n        \"Amharic\":amwo_acc,\n        \"Yoruba\":yowo_acc,   \n    }\n)\ndf.to_csv('F1.csv',index=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:20:53.441809Z","iopub.execute_input":"2022-04-19T18:20:53.442072Z","iopub.status.idle":"2022-04-19T18:20:53.448645Z","shell.execute_reply.started":"2022-04-19T18:20:53.442044Z","shell.execute_reply":"2022-04-19T18:20:53.447928Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"code","source":"df2=pd.DataFrame({\"Average F1\":[mean_og,mean_sw,mean_am,mean_yo],})\ndf2.index = ['Original', 'Swahili', 'Amharic', 'Yoruba']\ndf2.to_csv('Mean.csv',index=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T18:20:55.956519Z","iopub.execute_input":"2022-04-19T18:20:55.957009Z","iopub.status.idle":"2022-04-19T18:20:55.963095Z","shell.execute_reply.started":"2022-04-19T18:20:55.956970Z","shell.execute_reply":"2022-04-19T18:20:55.962457Z"},"trusted":true},"execution_count":212,"outputs":[]}]}